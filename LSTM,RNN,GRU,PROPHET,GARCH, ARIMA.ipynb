{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb64a50",
   "metadata": {},
   "source": [
    "### Utilizing Cryptocompare Website API for Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164ad5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://min-api.cryptocompare.com/data/histoday'\n",
    "res = requests.get(endpoint + '?fsym=BTC&tsym=CAD&limit=2000')\n",
    "hist = pd.DataFrame(json.loads(res.content)['Data'])\n",
    "hist = hist.set_index('time')\n",
    "hist.index = pd.to_datetime(hist.index, unit='s')\n",
    "target_col = 'close'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da5e8620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of                 high       low      open  volumefrom     volumeto     close  \\\n",
       "time                                                                          \n",
       "2018-03-16  11363.92  10682.89  10772.45      908.75   9938473.94  11031.87   \n",
       "2018-03-17  11529.53  10288.24  11031.62      713.03   7573045.24  10540.55   \n",
       "2018-03-18  11055.26   9687.76  10541.91     1307.06  13247879.83  10882.76   \n",
       "2018-03-19  11483.72  10767.82  10881.34     1153.03  12586272.22  11304.89   \n",
       "2018-03-20  11916.17  11032.43  11304.89      607.89   6789380.22  11760.75   \n",
       "...              ...       ...       ...         ...          ...       ...   \n",
       "2023-09-02  35220.93  34846.44  34994.67       19.69    689285.74  35078.23   \n",
       "2023-09-03  35441.36  34995.41  35078.23       21.00    738893.23  35297.26   \n",
       "2023-09-04  35721.67  34897.22  35297.26       24.62    866308.32  35035.46   \n",
       "2023-09-05  35351.72  34831.91  35035.46       45.67   1599588.58  35076.63   \n",
       "2023-09-06  35355.21  34670.71  35076.63       29.07   1017449.52  34902.88   \n",
       "\n",
       "           conversionType conversionSymbol  \n",
       "time                                        \n",
       "2018-03-16         direct                   \n",
       "2018-03-17         direct                   \n",
       "2018-03-18         direct                   \n",
       "2018-03-19         direct                   \n",
       "2018-03-20         direct                   \n",
       "...                   ...              ...  \n",
       "2023-09-02         direct                   \n",
       "2023-09-03         direct                   \n",
       "2023-09-04         direct                   \n",
       "2023-09-05         direct                   \n",
       "2023-09-06         direct                   \n",
       "\n",
       "[2001 rows x 8 columns]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf3237c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import time  # Add this import statement to use time.mktime()\n",
    "import requests  # Add this import statement to use requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57069dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def construct_download_url(\n",
    "    ticker,\n",
    "    period1,\n",
    "    period2,\n",
    "    interval='daily'\n",
    "\n",
    "):\n",
    "    def convert_to_seconds(period):\n",
    "        datetime_value = datetime.strptime(period, '%Y-%m-%d')\n",
    "        total_seconds = int(time.mktime(datetime_value.timetuple())) + 86400\n",
    "        return total_seconds\n",
    "\n",
    "    try:\n",
    "        interval_reference = {'daily': '1d', 'weekly': '1wk', 'monthly': '1mo'}\n",
    "        _interval = interval_reference.get(interval)\n",
    "        if _interval is None:\n",
    "            print('interval code is incorrect')\n",
    "            return\n",
    "        p1 = convert_to_seconds(period1)\n",
    "        p2 = convert_to_seconds(period2)\n",
    "        url = f'https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={p1}&period2={p2}&interval={_interval}&filter=history'\n",
    "        return url\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "#### Getting the Dataset online\n",
    "query_url = construct_download_url('BTC-USD', '2019-01-01', '2023-01-01', 'daily')\n",
    "\n",
    "# Make an HTTP GET request to download the data\n",
    "response = requests.get(query_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Convert the response content to a DataFrame\n",
    "    df = pd.read_csv(pd.compat.StringIO(response.text))\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    ### Saving dataset as CSV file;\n",
    "    df.to_csv('crypto_price_data.csv')\n",
    "\n",
    "    ##As JSON\n",
    " #   with open('BTC-USD.json', 'w') as f:\n",
    "  #      f.write(json.dumps(df.T.to_dict(), indent=4))\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1bcb09a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21342/3874011040.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Sort the data by date if it's not already sorted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mcrypto_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Calculate Moving Averages (e.g., 7-day and 30-day moving averages)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[1;32m   6320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6321\u001b[0m             \u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6322\u001b[0;31m             \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6324\u001b[0m             \u001b[0;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1838\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "### MAIN CODE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load cryptocurrency price data (assuming you have a CSV file)\n",
    "crypto_data = hist#pd.read_csv('crypto_price_data.csv')\n",
    "\n",
    "# Assuming your dataset has columns 'Date' and 'Close' for the date and closing prices\n",
    "# Convert the 'Date' column to datetime if it's not already in that format\n",
    "#crypto_data['Date'] = pd.to_datetime(crypto_data['Date'])\n",
    "\n",
    "# Sort the data by date if it's not already sorted\n",
    "crypto_data.sort_values(by='Date', inplace=True)\n",
    "\n",
    "# Calculate Moving Averages (e.g., 7-day and 30-day moving averages)\n",
    "crypto_data['7D_MA'] = crypto_data['Close'].rolling(window=7).mean()\n",
    "crypto_data['30D_MA'] = crypto_data['Close'].rolling(window=30).mean()\n",
    "\n",
    "# Calculate Rate of Change (ROC)\n",
    "n = 1  # You can adjust the lookback period\n",
    "crypto_data['ROC'] = (crypto_data['Close'] - crypto_data['Close'].shift(n)) / crypto_data['Close'].shift(n)\n",
    "\n",
    "# Calculate Relative Strength Index (RSI)\n",
    "def calculate_rsi(data, period=14):\n",
    "    delta = data['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "crypto_data['RSI'] = calculate_rsi(crypto_data)\n",
    "\n",
    "# Drop rows with NaN values (resulting from rolling calculations)\n",
    "crypto_data.dropna(inplace=True)\n",
    "\n",
    "# Optionally, you can save the feature-engineered data to a new CSV file\n",
    "# crypto_data.to_csv('crypto_price_data_with_features.csv', index=False)\n",
    "\n",
    "# Now you can use the crypto_data DataFrame for deep learning modeling\n",
    "#we load cryptocurrency price data from a CSV file, convert dates to datetime objects, sort the data by date, and then calculate various features such as moving averages (7-day and 30-day), rate of #change (ROC), and the Relative Strength Index (RSI). You can customize this code to add more features or adjust the lookback periods and parameters according to your modeling needs.\n",
    "\n",
    "#MARKET SENTIMENTAL ANALYSIS\n",
    "\n",
    "'''\n",
    "We load both cryptocurrency price data and market news sentiment data from CSV files.\n",
    "\n",
    "We merge the two datasets based on the publication date.\n",
    "\n",
    "We use the VADER sentiment analysis library to calculate sentiment scores for news headlines.\n",
    "\n",
    "We normalize the price and sentiment data to a common scale between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "We create sequences of input features and target values for the LSTM model.\n",
    "\n",
    "We split the data into training and testing sets and build an LSTM-based deep learning model for price prediction.\n",
    "\n",
    "The model is trained, evaluated, and used to make predictions\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Load cryptocurrency price data (assuming you have a CSV file)\n",
    "crypto_data = hist#pd.read_csv('crypto_price_data.csv')\n",
    "\n",
    "# Load market news sentiment data (assuming you have a CSV file with 'Date' and 'Headline' columns)\n",
    "news_data = pd.read_csv('market_news_sentiments.csv')\n",
    "\n",
    "# Convert the 'Date' columns to datetime objects\n",
    "#crypto_data['Date'] = pd.to_datetime(crypto_data['Date'])\n",
    "#news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
    "\n",
    "# Merge cryptocurrency price data with market news sentiment data\n",
    "merged_data = pd.merge(crypto_data, news_data, on='Date', how='left')\n",
    "\n",
    "# Fill missing sentiment values with neutral sentiment (0)\n",
    "merged_data['Sentiment'].fillna(0, inplace=True)\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for news headlines\n",
    "merged_data['Sentiment'] = merged_data['Headline'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Normalize price and sentiment data\n",
    "scaler = MinMaxScaler()\n",
    "merged_data[['Close', 'Sentiment']] = scaler.fit_transform(merged_data[['Close', 'Sentiment']])\n",
    "\n",
    "# Define the number of past days to consider for prediction\n",
    "lookback = 7\n",
    "\n",
    "# Create sequences of input features and target (next day's closing price)\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(merged_data) - lookback):\n",
    "    X.append(merged_data['Close'].values[i:i+lookback])\n",
    "    y.append(merged_data['Close'].values[i+lookback])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a deep learning model (LSTM-based in this example)\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape X_train and X_test for LSTM input\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "\n",
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the scaled predictions and actual values to get actual prices\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# You can now analyze the predictions and evaluate the model's performance\n",
    "In this code:\n",
    "\n",
    "We load both cryptocurrency price data and market news sentiment data from CSV files.\n",
    "\n",
    "We merge the two datasets based on the publication date.\n",
    "\n",
    "We use the VADER sentiment analysis library to calculate sentiment scores for news headlines.\n",
    "\n",
    "We normalize the price and sentiment data to a common scale between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "We create sequences of input features and target values for the LSTM model.\n",
    "\n",
    "We split the data into training and testing sets and build an LSTM-based deep learning model for price prediction.\n",
    "\n",
    "The model is trained, evaluated, and used to make predictions.\n",
    "\n",
    "Please note that this is a simplified example, and more advanced models and feature engineering techniques can be \n",
    "explored for better performance. Additionally, you may want to fine-tune hyperparameters and optimize the model \n",
    "further for your specific use case.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "User\n",
    "why do you need normalization\n",
    "ChatGPT\n",
    "Normalization is a crucial preprocessing step when working with numerical data, including cryptocurrency prices and \n",
    "sentiment scores, for several reasons:\n",
    "\n",
    "Scale Independence: Different features often have different scales. For example, cryptocurrency prices can have \n",
    "values in the thousands or tens of thousands, while sentiment scores typically range from -1 to 1. Deep learning\n",
    "models, particularly gradient-based optimization algorithms, are sensitive to the scale of input features. \n",
    "Normalization ensures that all features contribute more equally to the model's training, preventing certain features from dominating the learning process.\n",
    "\n",
    "Faster Convergence: Normalization helps the optimization process converge faster during model training.\n",
    "Without normalization, the optimization algorithm may oscillate or take longer to find an optimal solution,\n",
    "especially if the features have significantly different scales.\n",
    "\n",
    "Improved Generalization: Normalization can lead to better model generalization. When features are normalized to a \n",
    "common scale, the model is less likely to overfit to the training data and is better equipped to make accurate\n",
    "predictions on unseen data.\n",
    "\n",
    "Stable Gradient Descent: Gradient descent, a common optimization technique used in deep learning, relies on the \n",
    "magnitude of gradients to update model parameters. If features have different scales, the gradients can vary widely, leading to unstable updates and convergence issues. Normalization helps stabilize the gradient descent process.\n",
    "\n",
    "Interpretability: Normalized features are more interpretable. When features are on a similar scale, it's easier to \n",
    "compare the relative importance of each feature in making predictions.\n",
    "\n",
    "In the code example provided in the previous response, we used Min-Max scaling, a common form of normalization that\n",
    "transforms features to a range between 0 and 1. Other normalization techniques, such as z-score normalization\n",
    "(standardization), can also be used, depending on the characteristics of the data and the requirements of the \n",
    "deep learning model.\n",
    "\n",
    "Overall, normalization is a best practice in machine learning and deep learning to ensure that your model can\n",
    "learn effectively and make accurate predictions, especially when dealing with features that have varying scales.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "### NO NEWS DATASET YET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a09be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#LSTM RNN GRU PROPHET ARIMA GARCH\n",
    "\n",
    "'''\n",
    "STEPS TO BE DONE BEFORE DATA WRANGLING,\n",
    "\n",
    "This code calculates and adds Moving Averages (SMA), RSI, Stochastic Oscillator (%K and %D), and Bollinger Bands\n",
    "(Upper and Lower) as additional features to your cryptocurrency price data.\n",
    " After adding these features, you can proceed with data preprocessing, splitting, and model building as demonstrated in previous responses.\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf  # For downloading cryptocurrency price data\n",
    "\n",
    "# Download cryptocurrency price data from Yahoo Finance (e.g., Bitcoin)\n",
    "crypto_data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Calculate Moving Averages (e.g., 50-day and 200-day)\n",
    "crypto_data['SMA_50'] = crypto_data['Close'].rolling(window=50).mean()\n",
    "crypto_data['SMA_200'] = crypto_data['Close'].rolling(window=200).mean()\n",
    "\n",
    "# Calculate RSI (Relative Strength Index)\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['Close'].diff(1)\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "    avg_gain = gain.rolling(window=window).mean()\n",
    "    avg_loss = loss.rolling(window=window).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "crypto_data['RSI'] = calculate_rsi(crypto_data)\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "def calculate_stochastic_oscillator(data, window=14):\n",
    "    low_min = data['Low'].rolling(window=window).min()\n",
    "    high_max = data['High'].rolling(window=window).max()\n",
    "\n",
    "    k = 100 * ((data['Close'] - low_min) / (high_max - low_min))\n",
    "    d = k.rolling(window=3).mean()\n",
    "    \n",
    "    return k, d\n",
    "\n",
    "crypto_data['%K'], crypto_data['%D'] = calculate_stochastic_oscillator(crypto_data)\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "def calculate_bollinger_bands(data, window=20, num_std_dev=2):\n",
    "    rolling_mean = data['Close'].rolling(window=window).mean()\n",
    "    rolling_std = data['Close'].rolling(window=window).std()\n",
    "\n",
    "    upper_band = rolling_mean + (rolling_std * num_std_dev)\n",
    "    lower_band = rolling_mean - (rolling_std * num_std_dev)\n",
    "    \n",
    "    return upper_band, lower_band\n",
    "\n",
    "crypto_data['Upper_Band'], crypto_data['Lower_Band'] = calculate_bollinger_bands(crypto_data)\n",
    "\n",
    "# Drop rows with NaN values (resulting from rolling calculations)\n",
    "crypto_data.dropna(inplace=True)\n",
    "\n",
    "# Now, crypto_data DataFrame contains the original price data along with calculated indicators\n",
    "\n",
    "# You can use this data for training and testing your cryptocurrency price prediction model.\n",
    "# Further preprocessing, splitting, and model building steps would be similar to previous examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6c6074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_50',\n",
       "       'SMA_200', 'RSI', '%K', '%D', 'Upper_Band', 'Lower_Band'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827c3c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open          High          Low           Close         Adj Close     Volume       SMA_50        SMA_200       RSI        %K         %D         Upper_Band    Lower_Band  \n",
       "9151.183594   9230.983398   9100.824219   9159.040039   9159.040039   12252601475  9394.715820   8534.130850   51.256905  38.456920  36.030095  9394.314926   9028.320035     1\n",
       "34290.292969  35364.925781  33881.835938  35350.187500  35350.187500  20856685287  34332.460938  44702.747041  55.060698  99.754524  91.568809  35530.362655  30070.851798    1\n",
       "41501.480469  41847.164062  41038.097656  41441.164062  41441.164062  19652846215  42986.917031  49074.233018  79.802026  95.313459  81.800377  43086.622475  33966.390025    1\n",
       "41501.746094  42126.300781  40961.097656  41374.378906  41374.378906  27819532341  42130.383828  48060.225000  42.041989  51.436065  39.248710  47375.466998  37616.579486    1\n",
       "41551.269531  44092.601562  41444.582031  43790.894531  43790.894531  31141681925  46570.229609  45271.428047  40.454855  44.460337  26.013766  49509.912440  39638.447326    1\n",
       "                                                                                                                                                                             ..\n",
       "21233.609375  21520.914062  20964.585938  21502.337891  21502.337891  18372538715  27708.087188  38662.031602  25.827907  35.146312  30.737624  32755.320673  15231.466436    1\n",
       "21244.169922  22986.529297  21070.806641  22930.548828  22930.548828  31758955233  21846.982500  34450.014463  66.724648  71.907875  53.484192  24010.702039  19255.231554    1\n",
       "21285.056641  21345.376953  20920.191406  20926.486328  20926.486328  35082693210  19659.887422  23980.654609  64.155584  76.773672  88.905184  21595.908697  18677.035639    1\n",
       "21308.351562  23642.660156  21234.675781  22805.162109  22805.162109  71378606374  17358.056719  12293.149966  71.293961  86.095295  91.524117  21454.681220  16873.863507    1\n",
       "67549.734375  68530.335938  66382.062500  66971.828125  66971.828125  42357991721  55819.933359  45583.732578  68.144553  84.903180  89.533711  66315.955222  57918.311965    1\n",
       "Length: 897, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a664c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 22:24:18.967671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 22:24:19.143992: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-06 22:24:19.170717: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-09-06 22:24:19.170728: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-09-06 22:24:19.831536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 22:24:19.831584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-06 22:24:19.831586: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3628\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3629\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21342/757057120.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3505\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3506\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3629\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3630\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3631\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3632\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "\n",
    "##############   LSTM\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load cryptocurrency price data\n",
    "#data = pd.read_csv('crypto_price_data.csv')  # Replace with your data file\n",
    "data = crypto_data  # Replace with your data file\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "data['Price'] = scaler.fit_transform(data['Price'].values.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target_val = data.iloc[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        target.append(target_val)\n",
    "    return np.array(sequences), np.array(target)\n",
    "\n",
    "seq_length = 10  # You can adjust this sequence length\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "# Build an LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "train_rmse = np.sqrt(mean_squared_error(train_data.iloc[seq_length:], train_predictions))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_data.iloc[seq_length:], test_predictions))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[seq_length:len(train_data)], train_predictions, label='Train Predictions', color='blue')\n",
    "plt.plot(data.index[len(train_data) + seq_length:], test_predictions, label='Test Predictions', color='red')\n",
    "plt.plot(data.index, data['Price'], label='Actual Price', color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e521fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############   RNN\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load cryptocurrency price data\n",
    "data = pd.read_csv('crypto_price_data.csv')  # Replace with your data file\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "data['Price'] = scaler.fit_transform(data['Price'].values.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target_val = data.iloc[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        target.append(target_val)\n",
    "    return np.array(sequences), np.array(target)\n",
    "\n",
    "seq_length = 10  # You can adjust this sequence length\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "# Build an RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "train_rmse = np.sqrt(mean_squared_error(train_data.iloc[seq_length:], train_predictions))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_data.iloc[seq_length:], test_predictions))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[seq_length:len(train_data)], train_predictions, label='Train Predictions', color='blue')\n",
    "plt.plot(data.index[len(train_data) + seq_length:], test_predictions, label='Test Predictions', color='red')\n",
    "plt.plot(data.index, data['Price'], label='Actual Price', color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f656a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##########   GRU MODEL\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load cryptocurrency price data\n",
    "data = pd.read_csv('crypto_price_data.csv')  # Replace with your data file\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "data['Price'] = scaler.fit_transform(data['Price'].values.reshape(-1, 1))\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(data) * 0.8)\n",
    "train_data, test_data = data.iloc[:train_size], data.iloc[train_size:]\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        target_val = data.iloc[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        target.append(target_val)\n",
    "    return np.array(sequences), np.array(target)\n",
    "\n",
    "seq_length = 10  # You can adjust this sequence length\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "# Build a GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(units=50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(units=1))\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "train_rmse = np.sqrt(mean_squared_error(train_data.iloc[seq_length:], train_predictions))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_data.iloc[seq_length:], test_predictions))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse}')\n",
    "print(f'Test RMSE: {test_rmse}')\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index[seq_length:len(train_data)], train_predictions, label='Train Predictions', color='blue')\n",
    "plt.plot(data.index[len(train_data) + seq_length:], test_predictions, label='Test Predictions', color='red')\n",
    "plt.plot(data.index, data['Price'], label='Actual Price', color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ab910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#################   FACEBOOK PROPHET\n",
    "\n",
    "In this code:\n",
    "\n",
    "We load the cryptocurrency price data into a Pandas DataFrame, ensuring that the date column is named 'ds' and\n",
    "the price column is named 'y'.\n",
    "\n",
    "We initialize a Prophet model and fit it to the data.\n",
    "\n",
    "We create a dataframe with future dates for which we want to make predictions. You can adjust the periods parameter\n",
    "to specify how far into the future you want to forecast.\n",
    "\n",
    "We use the predict method to generate predictions for the future dates.\n",
    "\n",
    "Finally, we plot the forecasted prices using Matplotlib.\n",
    "\n",
    "Keep in mind that Prophet is a powerful tool for time series forecasting and can handle various types of time series\n",
    "data.\n",
    " You can further fine-tune the model by adding holidays, adjusting seasonality settings, and working with additional\n",
    " regressors if you have relevant data.\n",
    "\n",
    " '''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load cryptocurrency price data\n",
    "data = pd.read_csv('crypto_price_data.csv')  # Replace with your data file\n",
    "\n",
    "# Preprocess data\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.rename(columns={'Date': 'ds', 'Price': 'y'})\n",
    "\n",
    "# Initialize and fit the Prophet model\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# Create a dataframe with future dates for prediction\n",
    "future = model.make_future_dataframe(periods=365)  # You can adjust the prediction horizon\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Plot the forecast\n",
    "fig = model.plot(forecast)\n",
    "plt.title('Cryptocurrency Price Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277b018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############   ARIMA\n",
    "\n",
    "\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(data, order=(p, d, q))\n",
    "results = model.fit()\n",
    "\n",
    "# Make predictions\n",
    "forecast, stderr, conf_int = results.forecast(steps=n_steps)\n",
    "\n",
    "\n",
    "'''\n",
    "ARIMA (AutoRegressive Integrated Moving Average) Model:\n",
    "Data Preprocessing:\n",
    "\n",
    "Load your cryptocurrency price data and convert it into a time series format. Ensure the data is stationary \n",
    "(i.e., the mean and variance do not vary significantly over time) by differencing or \n",
    "other transformations if necessary.\n",
    "Exploratory Data Analysis (EDA):\n",
    "\n",
    "Examine your time series data using plots like time series plots,\n",
    " autocorrelation plots (ACF), and partial autocorrelation plots (PACF) to determine the order of differencing (d),\n",
    " autoregressive (p), and moving average (q) terms.\n",
    "Model Fitting:\n",
    "\n",
    "Fit an ARIMA model to your preprocessed data using the determined values of p, d, and q. You can use libraries like \n",
    "statsmodels in Python for this purpose.\n",
    "Model Evaluation:\n",
    "Evaluate the ARIMA model using metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), or others\n",
    "appropriate for your use case.\n",
    "Prediction:\n",
    "Use the trained ARIMA model to make future price predictions.\n",
    "'''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Download cryptocurrency price data from Yahoo Finance (e.g., Bitcoin)\n",
    "crypto_data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Preprocess the data and make it stationary (if necessary)\n",
    "# For example, you can difference the data to remove trend\n",
    "crypto_data['Diff'] = crypto_data['Close'].diff()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = crypto_data['Diff'][:'2022-01-01']\n",
    "test_data = crypto_data['Diff']['2022-01-01':]\n",
    "\n",
    "# Fit an ARIMA model\n",
    "model = ARIMA(train_data, order=(p, d, q))  # Define appropriate values for p, d, and q\n",
    "model_fit = model.fit(disp=0)\n",
    "\n",
    "# Make predictions\n",
    "n_forecast = len(test_data)\n",
    "forecast, stderr, conf_int = model_fit.forecast(steps=n_forecast)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "# You can use metrics like Mean Squared Error (MSE), Root Mean Squared Error (RMSE), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af85239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######  GARCH\n",
    "\n",
    "\n",
    "\n",
    "#GARCH (Generalized Autoregressive Conditional Heteroskedasticity) Model:\n",
    "#Data Preprocessing:\n",
    "\n",
    "#Similarly, preprocess your cryptocurrency price data and ensure it is stationary.\n",
    "#Exploratory Data Analysis (EDA):\n",
    "\n",
    "#Examine the data for signs of volatility clustering or ARCH (Autoregressive Conditional Heteroskedasticity) effects.\n",
    "#Model Fitting:\n",
    "#Fit a GARCH model to your preprocessed data. You can use libraries like arch in Python.\n",
    "\n",
    "\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "# Fit GARCH model\n",
    "model = arch_model(data, vol='Garch', p=p, q=q)\n",
    "results = model.fit()\n",
    "\n",
    "# Forecast volatility\n",
    "forecast_volatility = results.conditional_volatility[-n_steps:]\n",
    "\n",
    "\n",
    "'''\n",
    "Model Evaluation:\n",
    "\n",
    "Evaluate the GARCH model using appropriate metrics. GARCH models typically focus on modeling volatility,\n",
    "so you might want to assess how well it captures volatility patterns.\n",
    "Volatility Forecasting:\n",
    "\n",
    "Use the trained GARCH model to forecast future volatility, which can be useful for risk management in \n",
    "cryptocurrency trading.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85135c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####  EXPONENTIAL SMOOTHING\n",
    "#Exponential Smoothing models, such as Holt-Winters, are suitable for time series data with trend and seasonality. To use Exponential Smoothing:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Download cryptocurrency price data from Yahoo Finance (e.g., Bitcoin)\n",
    "crypto_data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Preprocess the data if necessary\n",
    "# For example, you can remove outliers or fill missing values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = crypto_data[:'2022-01-01']\n",
    "test_data = crypto_data['2022-01-01':]\n",
    "\n",
    "# Fit an Exponential Smoothing model\n",
    "model = ExponentialSmoothing(train_data['Close'], seasonal='add', seasonal_periods=7)  # Adjust parameters as needed\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Make predictions\n",
    "n_forecast = len(test_data)\n",
    "forecast = model_fit.forecast(steps=n_forecast)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "# Use appropriate metrics such as MSE, RMSE, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b4e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d195ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address1': 'One Microsoft Way', 'city': 'Redmond', 'state': 'WA', 'zip': '98052-6399', 'country': 'United States', 'phone': '425 882 8080', 'website': 'https://www.microsoft.com', 'industry': 'Software—Infrastructure', 'industryDisp': 'Software—Infrastructure', 'sector': 'Technology', 'sectorDisp': 'Technology', 'longBusinessSummary': 'Microsoft Corporation develops and supports software, services, devices and solutions worldwide. The Productivity and Business Processes segment offers office, exchange, SharePoint, Microsoft Teams, office 365 Security and Compliance, Microsoft viva, and Microsoft 365 copilot; and office consumer services, such as Microsoft 365 consumer subscriptions, Office licensed on-premises, and other office services. This segment also provides LinkedIn; and dynamics business solutions, including Dynamics 365, a set of intelligent, cloud-based applications across ERP, CRM, power apps, and power automate; and on-premises ERP and CRM applications. The Intelligent Cloud segment provides server products and cloud services, such as azure and other cloud services; SQL and windows server, visual studio, system center, and related client access licenses, as well as nuance and GitHub; and enterprise services including enterprise support services, industry solutions, and nuance professional services. The More Personal Computing segment offers Windows, including windows OEM licensing and other non-volume licensing of the Windows operating system; Windows commercial comprising volume licensing of the Windows operating system, windows cloud services, and other Windows commercial offerings; patent licensing; and windows Internet of Things; and devices, such as surface, HoloLens, and PC accessories. Additionally, this segment provides gaming, which includes Xbox hardware and content, and first- and third-party content; Xbox game pass and other subscriptions, cloud gaming, advertising, third-party disc royalties, and other cloud services; and search and news advertising, which includes Bing, Microsoft News and Edge, and third-party affiliates. The company sells its products through OEMs, distributors, and resellers; and directly through digital marketplaces, online, and retail stores. The company was founded in 1975 and is headquartered in Redmond, Washington.', 'fullTimeEmployees': 221000, 'companyOfficers': [{'maxAge': 1, 'name': 'Mr. Satya  Nadella', 'age': 55, 'title': 'Chairman & CEO', 'yearBorn': 1967, 'fiscalYear': 2022, 'totalPay': 12676750, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Bradford L. Smith LCA', 'age': 63, 'title': 'Pres & Vice Chairman', 'yearBorn': 1959, 'fiscalYear': 2022, 'totalPay': 4655274, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Amy E. Hood', 'age': 50, 'title': 'Exec. VP & CFO', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4637915, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Judson  Althoff', 'age': 49, 'title': 'Exec. VP & Chief Commercial Officer', 'yearBorn': 1973, 'fiscalYear': 2022, 'totalPay': 4428268, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher David Young', 'age': 50, 'title': 'Exec. VP of Bus. Devel., Strategy & Ventures', 'yearBorn': 1972, 'fiscalYear': 2022, 'totalPay': 4588876, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Ms. Alice L. Jolla', 'age': 55, 'title': 'Corp. VP & Chief Accounting Officer', 'yearBorn': 1967, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. James Kevin Scott', 'age': 50, 'title': 'Exec. VP of AI & CTO', 'yearBorn': 1972, 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Brett  Iversen', 'title': 'Gen. Mang. of Investor Relations', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Frank X. Shaw', 'title': 'Corp. VP for Corp. Communications', 'exercisedValue': 0, 'unexercisedValue': 0}, {'maxAge': 1, 'name': 'Mr. Christopher C. Capossela', 'age': 52, 'title': 'Exec. VP & Chief Marketing Officer', 'yearBorn': 1970, 'exercisedValue': 0, 'unexercisedValue': 0}], 'auditRisk': 6, 'boardRisk': 3, 'compensationRisk': 3, 'shareHolderRightsRisk': 2, 'overallRisk': 2, 'governanceEpochDate': 1693526400, 'compensationAsOfEpochDate': 1672444800, 'maxAge': 86400, 'priceHint': 2, 'previousClose': 333.55, 'open': 333.38, 'dayLow': 330.18, 'dayHigh': 334.46, 'regularMarketPreviousClose': 333.55, 'regularMarketOpen': 333.38, 'regularMarketDayLow': 330.18, 'regularMarketDayHigh': 334.46, 'dividendRate': 2.72, 'dividendYield': 0.0082, 'exDividendDate': 1692144000, 'payoutRatio': 0.2748, 'fiveYearAvgDividendYield': 1.05, 'beta': 0.904564, 'trailingPE': 34.352943, 'forwardPE': 26.37718, 'volume': 17355285, 'regularMarketVolume': 17355285, 'averageVolume': 25963837, 'averageVolume10days': 19286450, 'averageDailyVolume10Day': 19286450, 'bid': 0.0, 'ask': 0.0, 'bidSize': 1200, 'askSize': 2200, 'marketCap': 2473218539520, 'fiftyTwoWeekLow': 213.43, 'fiftyTwoWeekHigh': 366.78, 'priceToSalesTrailing12Months': 11.670805, 'fiftyDayAverage': 332.7632, 'twoHundredDayAverage': 289.49585, 'trailingAnnualDividendRate': 2.72, 'trailingAnnualDividendYield': 0.0081547, 'currency': 'USD', 'enterpriseValue': 2441404743680, 'profitMargins': 0.34146, 'floatShares': 7423448423, 'sharesOutstanding': 7429760000, 'sharesShort': 31028035, 'sharesShortPriorMonth': 42497800, 'sharesShortPreviousMonthDate': 1689292800, 'dateShortInterest': 1692057600, 'sharesPercentSharesOut': 0.0042, 'heldPercentInsiders': 0.00052, 'heldPercentInstitutions': 0.73212, 'shortRatio': 1.0, 'shortPercentOfFloat': 0.0042, 'impliedSharesOutstanding': 7429760000, 'bookValue': 27.748, 'priceToBook': 11.996541, 'lastFiscalYearEnd': 1688083200, 'nextFiscalYearEnd': 1719705600, 'mostRecentQuarter': 1688083200, 'earningsQuarterlyGrowth': 0.2, 'netIncomeToCommon': 72361000960, 'trailingEps': 9.69, 'forwardEps': 12.62, 'pegRatio': 2.11, 'lastSplitFactor': '2:1', 'lastSplitDate': 1045526400, 'enterpriseToRevenue': 11.521, 'enterpriseToEbitda': 23.93, '52WeekChange': 0.28763735, 'SandP52WeekChange': 0.114647865, 'lastDividendValue': 0.68, 'lastDividendDate': 1692144000, 'exchange': 'NMS', 'quoteType': 'EQUITY', 'symbol': 'MSFT', 'underlyingSymbol': 'MSFT', 'shortName': 'Microsoft Corporation', 'longName': 'Microsoft Corporation', 'firstTradeDateEpochUtc': 511108200, 'timeZoneFullName': 'America/New_York', 'timeZoneShortName': 'EDT', 'uuid': 'b004b3ec-de24-385e-b2c1-923f10d3fb62', 'messageBoardId': 'finmb_21835', 'gmtOffSetMilliseconds': -14400000, 'currentPrice': 332.88, 'targetHighPrice': 440.0, 'targetLowPrice': 232.0, 'targetMeanPrice': 387.17, 'targetMedianPrice': 400.0, 'recommendationMean': 1.8, 'recommendationKey': 'buy', 'numberOfAnalystOpinions': 44, 'totalCash': 111256002560, 'totalCashPerShare': 14.974, 'ebitda': 102022995968, 'totalDebt': 79441002496, 'quickRatio': 1.536, 'currentRatio': 1.769, 'totalRevenue': 211914997760, 'debtToEquity': 38.522, 'revenuePerShare': 28.46, 'returnOnAssets': 0.14245, 'returnOnEquity': 0.38824, 'freeCashflow': 47268999168, 'operatingCashflow': 87581999104, 'earningsGrowth': 0.202, 'revenueGrowth': 0.083, 'grossMargins': 0.6892, 'ebitdaMargins': 0.48143002, 'operatingMargins': 0.41772997, 'financialCurrency': 'USD', 'trailingPegRatio': 2.4372}\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "msft = yf.Ticker(\"MSFT\")\n",
    "print(msft.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d548d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db7e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
